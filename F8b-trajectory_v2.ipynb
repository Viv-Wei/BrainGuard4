{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c23da-9c52-46b5-bce1-4c9f01246e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Longitudinal trajectory clustering analysis of behavior_score (with covariate adjustment)\n",
    "- Fixed number of clusters: 3\n",
    "- Re-order group labels based on mean behavior_score (low → high = 0 → 2)\n",
    "- Output wide-format data\n",
    "- Analyze association between group and dementia (dementia_final)\n",
    "- Automatically detect CSV delimiter\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "base_dir = r\"./data/F8/\"\n",
    "output_dir = r\"./plot/F8/\"\n",
    "\n",
    "main_csv = os.path.join(base_dir, \"trajectory_charls.csv\")  # Replace with other cohort data if needed\n",
    "\n",
    "# Dataset name (used in output filenames)\n",
    "DATASET_NAME = \"CHARLS\"\n",
    "\n",
    "# ★ Score variable name\n",
    "SCORE_VAR = \"behavior_score\"\n",
    "\n",
    "# =========================\n",
    "# ★ Fixed number of clusters = 3\n",
    "# =========================\n",
    "FIXED_K = 3\n",
    "\n",
    "# =========================\n",
    "# ★ Function to auto-detect CSV delimiter\n",
    "# =========================\n",
    "def read_csv_auto(filepath, name=\"self_data\"):\n",
    "    \"\"\"Automatically detect delimiter and read CSV file\"\"\"\n",
    "    print(f\"\\n--- Reading {name}: {os.path.basename(filepath)} ---\")\n",
    "    \n",
    "    # Peek at the first line\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline()\n",
    "    \n",
    "    # Detect delimiter\n",
    "    if '\\t' in first_line and ',' not in first_line:\n",
    "        sep = '\\t'\n",
    "        sep_name = \"tab\"\n",
    "    elif ',' in first_line:\n",
    "        sep = ','\n",
    "        sep_name = \"comma\"\n",
    "    elif ';' in first_line:\n",
    "        sep = ';'\n",
    "        sep_name = \"semicolon\"\n",
    "    else:\n",
    "        sep = ','\n",
    "        sep_name = \"default comma\"\n",
    "    \n",
    "    print(f\"  Detected delimiter: {sep_name}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath, sep=sep)\n",
    "    print(f\"  Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "    print(f\"  Column names: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "print(\"=== Loading data ===\")\n",
    "\n",
    "# Main dataset (single file)\n",
    "df = read_csv_auto(main_csv, \"main dataset\")\n",
    "\n",
    "# Check for dementia_status\n",
    "if 'dementia_status' in df.columns:\n",
    "    print(f\"  ★ dementia_status found. Distribution: {df['dementia_status'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(\"  ⚠ dementia_status not found in data\")\n",
    "\n",
    "# Check score variable\n",
    "if SCORE_VAR in df.columns:\n",
    "    print(f\"  ★ {SCORE_VAR} found\")\n",
    "else:\n",
    "    print(f\"  ⚠ {SCORE_VAR} not found! Please check column names\")\n",
    "\n",
    "# =========================\n",
    "# Data preprocessing\n",
    "# =========================\n",
    "print(\"\\n=== Data preprocessing ===\")\n",
    "\n",
    "df['id'] = df['id'].astype(str)\n",
    "df['wave'] = pd.to_numeric(df['wave'], errors='coerce')\n",
    "\n",
    "# Record wave range of the cohort\n",
    "WAVE_MIN = df['wave'].min()\n",
    "WAVE_MAX = df['wave'].max()\n",
    "print(f\"\\n★ Cohort wave range: {WAVE_MIN} ~ {WAVE_MAX}\")\n",
    "\n",
    "# =========================\n",
    "# Data cleaning\n",
    "# =========================\n",
    "print(\"\\n=== Data cleaning ===\")\n",
    "\n",
    "# ★ Covariates used for adjustment: age, sex, bmi\n",
    "numeric_cols = [SCORE_VAR, 'age', 'bmi', 'dementia_status']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "key_cols = ['id', 'wave', SCORE_VAR]\n",
    "df = df.dropna(subset=key_cols).copy()\n",
    "print(f\"After removing missing {SCORE_VAR}: {len(df)} rows\")\n",
    "\n",
    "# =========================\n",
    "# Handle duplicates\n",
    "# =========================\n",
    "def mode_or_first(x):\n",
    "    m = x.mode()\n",
    "    return m.iloc[0] if len(m) > 0 else x.iloc[0]\n",
    "\n",
    "agg_dict = {SCORE_VAR: 'mean'}\n",
    "if 'age' in df.columns:\n",
    "    agg_dict['age'] = 'mean'\n",
    "if 'sex' in df.columns:\n",
    "    agg_dict['sex'] = mode_or_first\n",
    "if 'bmi' in df.columns:\n",
    "    agg_dict['bmi'] = 'mean'\n",
    "if 'dementia_status' in df.columns:\n",
    "    agg_dict['dementia_status'] = 'max'\n",
    "\n",
    "df = df.groupby(['id', 'wave'], as_index=False).agg(agg_dict)\n",
    "print(f\"After duplicate handling: {len(df)} rows\")\n",
    "\n",
    "measurement_counts = df.groupby('id').size()\n",
    "ids_with_at_least_2 = measurement_counts[measurement_counts >= 2].index\n",
    "df = df[df['id'].isin(ids_with_at_least_2)].copy()\n",
    "print(f\"IDs with ≥2 measurements: {len(ids_with_at_least_2)} individuals\")\n",
    "print(f\"Final dataset: {len(df)} rows\")\n",
    "\n",
    "df = df.sort_values(['id', 'wave']).reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# Covariate adjustment\n",
    "# =========================\n",
    "print(\"\\n=== Covariate adjustment ===\")\n",
    "\n",
    "# ★ Adjust only for age, sex, bmi\n",
    "adjustment_vars = ['age', 'sex', 'bmi']\n",
    "available_adj_vars = [v for v in adjustment_vars if v in df.columns]\n",
    "print(f\"Adjustment covariates: {available_adj_vars}\")\n",
    "\n",
    "df_for_regression = df.dropna(subset=available_adj_vars + [SCORE_VAR]).copy()\n",
    "print(f\"After removing covariate missing values: {len(df_for_regression)} rows, \"\n",
    "      f\"{df_for_regression['id'].nunique()} individuals\")\n",
    "\n",
    "transformers = []\n",
    "if 'sex' in available_adj_vars:\n",
    "    transformers.append(('sex_onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore'), ['sex']))\n",
    "\n",
    "ct = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "\n",
    "X = ct.fit_transform(df_for_regression[available_adj_vars])\n",
    "y = df_for_regression[SCORE_VAR].values\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "y_pred = linreg.predict(X)\n",
    "\n",
    "df_for_regression[f'{SCORE_VAR}_adj'] = y - y_pred\n",
    "\n",
    "print(f\"Adjustment model R² = {linreg.score(X, y):.4f}\")\n",
    "\n",
    "df = df_for_regression.copy()\n",
    "\n",
    "# =========================\n",
    "# Interpolate to uniform time grid\n",
    "# =========================\n",
    "print(\"\\n=== Trajectory feature extraction ===\")\n",
    "\n",
    "time_grid = np.linspace(WAVE_MIN, WAVE_MAX, num=20)\n",
    "\n",
    "def interpolate_trajectory(group, score_col):\n",
    "    if len(group) < 2:\n",
    "        return np.full(len(time_grid), group[score_col].values[0])\n",
    "    f = interp1d(group['wave'], group[score_col], kind='linear', fill_value='extrapolate')\n",
    "    return f(time_grid)\n",
    "\n",
    "unique_ids = df['id'].unique()\n",
    "\n",
    "interpolated_original = []\n",
    "interpolated_adjusted = []\n",
    "smoothed_trajectories = []\n",
    "derivative_features = []\n",
    "\n",
    "for id_val in unique_ids:\n",
    "    g = df[df['id'] == id_val].sort_values('wave')\n",
    "    \n",
    "    interp_orig = interpolate_trajectory(g, SCORE_VAR)\n",
    "    interpolated_original.append(interp_orig)\n",
    "    \n",
    "    interp_adj = interpolate_trajectory(g, f'{SCORE_VAR}_adj')\n",
    "    interpolated_adjusted.append(interp_adj)\n",
    "\n",
    "    if len(interp_adj) >= 5:\n",
    "        smoothed = savgol_filter(interp_adj, window_length=5, polyorder=2)\n",
    "    else:\n",
    "        smoothed = interp_adj\n",
    "\n",
    "    deriv = np.gradient(smoothed, time_grid)\n",
    "    smoothed_trajectories.append(smoothed)\n",
    "    derivative_features.append(deriv)\n",
    "\n",
    "interpolated_original = np.array(interpolated_original)\n",
    "interpolated_adjusted = np.array(interpolated_adjusted)\n",
    "X_smoothed = np.array(smoothed_trajectories)\n",
    "X_deriv = np.array(derivative_features)\n",
    "X_feat = np.hstack([X_smoothed, X_deriv])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_feat)\n",
    "\n",
    "print(f\"Feature matrix: {X_scaled.shape[0]} individuals × {X_scaled.shape[1]} features\")\n",
    "\n",
    "# =========================\n",
    "# Hierarchical clustering\n",
    "# =========================\n",
    "print(\"\\n=== Hierarchical clustering (fixed k=3) ===\")\n",
    "\n",
    "linkage_matrix = linkage(X_scaled, method='ward', metric='euclidean')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=30, leaf_rotation=90)\n",
    "plt.title(f'Dendrogram - {DATASET_NAME} {SCORE_VAR} Trajectory Clustering', fontsize=14)\n",
    "plt.xlabel('Sample Index / Cluster')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'dendrogram_{DATASET_NAME}_{SCORE_VAR}_adjusted_k3.pdf'),\n",
    "            format='pdf', bbox_inches='tight')\n",
    "plt.savefig(os.path.join(output_dir, f'dendrogram_{DATASET_NAME}_{SCORE_VAR}_adjusted_k3.png'),\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSilhouette scores for reference:\")\n",
    "k_range = range(2, 7)\n",
    "sil_scores = []\n",
    "for k in k_range:\n",
    "    model_k = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    labels_k = model_k.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels_k)\n",
    "    sil_scores.append((k, sil))\n",
    "    marker = \" ← selected\" if k == FIXED_K else \"\"\n",
    "    print(f\"  k={k}: Silhouette = {sil:.4f}{marker}\")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters=FIXED_K, linkage='ward')\n",
    "cluster_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "# =========================\n",
    "# ★ Re-order groups by mean behavior_score\n",
    "# =========================\n",
    "print(f\"\\n=== ★ Re-order groups by mean {SCORE_VAR} ===\")\n",
    "\n",
    "group_means = {}\n",
    "for g in range(FIXED_K):\n",
    "    group_ids = unique_ids[cluster_labels == g]\n",
    "    group_data = df[df['id'].isin(group_ids)]\n",
    "    group_means[g] = group_data[SCORE_VAR].mean()\n",
    "\n",
    "print(\"Original group means:\")\n",
    "for g, m in sorted(group_means.items()):\n",
    "    print(f\"  Original Group {g}: mean {SCORE_VAR} = {m:.3f}\")\n",
    "\n",
    "sorted_groups = sorted(group_means.keys(), key=lambda x: group_means[x])\n",
    "old_to_new = {old: new for new, old in enumerate(sorted_groups)}\n",
    "\n",
    "print(\"\\nRemapping:\")\n",
    "for old in sorted(old_to_new.keys()):\n",
    "    new = old_to_new[old]\n",
    "    print(f\"  Original Group {old} → New Group {new}\")\n",
    "\n",
    "cluster_labels = np.array([old_to_new[g] for g in cluster_labels])\n",
    "\n",
    "print(\"\\n★ After re-ordering:\")\n",
    "print(\"  Group 0 = Low level (low protection → high dementia risk)\")\n",
    "print(\"  Group 1 = Medium level\")\n",
    "print(\"  Group 2 = High level (high protection → low dementia risk, reference)\")\n",
    "\n",
    "# =========================\n",
    "# Write group labels back\n",
    "# =========================\n",
    "id_to_group = dict(zip(unique_ids, cluster_labels))\n",
    "df['group'] = df['id'].map(id_to_group)\n",
    "\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(\"\\nCluster sizes:\")\n",
    "for g, n in cluster_sizes.items():\n",
    "    print(f\"  Group {g}: n = {n} ({n/len(cluster_labels)*100:.1f}%)\")\n",
    "\n",
    "# =========================\n",
    "# Generate wide-format data\n",
    "# =========================\n",
    "print(\"\\n=== Generating wide-format data ===\")\n",
    "\n",
    "all_waves = sorted(df['wave'].unique())\n",
    "\n",
    "# Pivot behavior_score to wide format\n",
    "df_score_wide = df.pivot(index='id', columns='wave', values=SCORE_VAR)\n",
    "df_score_wide.columns = [f'{SCORE_VAR}_wave{int(w)}' for w in df_score_wide.columns]\n",
    "df_score_wide = df_score_wide.reset_index()\n",
    "\n",
    "# Pivot adjusted score to wide format\n",
    "df_score_adj_wide = df.pivot(index='id', columns='wave', values=f'{SCORE_VAR}_adj')\n",
    "df_score_adj_wide.columns = [f'{SCORE_VAR}_adj_wave{int(w)}' for w in df_score_adj_wide.columns]\n",
    "df_score_adj_wide = df_score_adj_wide.reset_index()\n",
    "\n",
    "# Baseline data\n",
    "df_baseline = df.sort_values(['id', 'wave']).groupby('id').first().reset_index()\n",
    "baseline_cols = ['id', 'group']\n",
    "for var in available_adj_vars:\n",
    "    if var in df_baseline.columns:\n",
    "        baseline_cols.append(var)\n",
    "df_baseline_selected = df_baseline[baseline_cols].copy()\n",
    "\n",
    "rename_dict = {var: f'{var}_baseline' for var in available_adj_vars if var in df_baseline_selected.columns}\n",
    "df_baseline_selected = df_baseline_selected.rename(columns=rename_dict)\n",
    "\n",
    "# ★ dementia_status (take max per person)\n",
    "print(\"\\n--- Extracting dementia_status ---\")\n",
    "if 'dementia_status' in df.columns:\n",
    "    df_dementia = df.groupby('id')['dementia_status'].max().reset_index()\n",
    "    df_dementia = df_dementia[df_dementia['id'].isin(unique_ids)]\n",
    "    print(f\"  dementia_status extracted: {len(df_dementia)} individuals\")\n",
    "    print(f\"  Distribution: {df_dementia['dementia_status'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    df_dementia = pd.DataFrame({'id': unique_ids})\n",
    "    print(\"  ⚠ dementia_status not found in data\")\n",
    "\n",
    "# Trajectory features\n",
    "trajectory_features = []\n",
    "for i, id_val in enumerate(unique_ids):\n",
    "    g = df[df['id'] == id_val].sort_values('wave')\n",
    "    trajectory_features.append({\n",
    "        'id': id_val,\n",
    "        'n_measurements': len(g),\n",
    "        'wave_min': g['wave'].min(),\n",
    "        'wave_max': g['wave'].max(),\n",
    "        'follow_up_duration': g['wave'].max() - g['wave'].min(),\n",
    "        f'{SCORE_VAR}_mean': g[SCORE_VAR].mean(),\n",
    "        f'{SCORE_VAR}_std': g[SCORE_VAR].std(),\n",
    "        f'{SCORE_VAR}_first': g[SCORE_VAR].iloc[0],\n",
    "        f'{SCORE_VAR}_last': g[SCORE_VAR].iloc[-1],\n",
    "        f'{SCORE_VAR}_change': g[SCORE_VAR].iloc[-1] - g[SCORE_VAR].iloc[0],\n",
    "    })\n",
    "\n",
    "df_traj_features = pd.DataFrame(trajectory_features)\n",
    "\n",
    "# Merge all\n",
    "df_wide = df_baseline_selected.merge(df_traj_features, on='id', how='left')\n",
    "if 'dementia_status' in df_dementia.columns:\n",
    "    df_wide = df_wide.merge(df_dementia, on='id', how='left')\n",
    "df_wide = df_wide.merge(df_score_wide, on='id', how='left')\n",
    "df_wide = df_wide.merge(df_score_adj_wide, on='id', how='left')\n",
    "\n",
    "print(f\"Wide-format data: {len(df_wide)} rows × {len(df_wide.columns)} columns\")\n",
    "print(f\"dementia_status present: {'dementia_status' in df_wide.columns}\")\n",
    "\n",
    "out_wide_csv = os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_with_trajectory_group_adjusted_k3.csv')\n",
    "df_wide.to_csv(out_wide_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"Saved: {out_wide_csv}\")\n",
    "\n",
    "# =========================\n",
    "# ★★★ Association between Group and Dementia ★★★\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ★ Association between Group and Dementia (dementia_status) ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'dementia_status' not in df_wide.columns:\n",
    "    print(\"\\n⚠ Error: dementia_status variable not found! Skipping dementia analysis\")\n",
    "else:\n",
    "    df_analysis = df_wide.dropna(subset=['group', 'dementia_status']).copy()\n",
    "    df_analysis['dementia_status'] = df_analysis['dementia_status'].astype(int)\n",
    "    df_analysis['group'] = df_analysis['group'].astype(int)\n",
    "    \n",
    "    print(f\"\\nAnalysis sample size: {len(df_analysis)} individuals\")\n",
    "    print(f\"Dementia events: {df_analysis['dementia_status'].sum()} ({df_analysis['dementia_status'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # 1. Dementia incidence by group\n",
    "    print(\"\\n--- 1. Dementia incidence by group ---\")\n",
    "    dementia_by_group = df_analysis.groupby('group').agg({\n",
    "        'dementia_status': ['count', 'sum', 'mean']\n",
    "    }).round(4)\n",
    "    dementia_by_group.columns = ['n', 'dementia_cases', 'dementia_rate']\n",
    "    dementia_by_group['dementia_pct'] = (dementia_by_group['dementia_rate'] * 100).round(2)\n",
    "    print(dementia_by_group)\n",
    "    \n",
    "    # 2. Chi-square test\n",
    "    print(\"\\n--- 2. Chi-square test ---\")\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    contingency_table = pd.crosstab(df_analysis['group'], df_analysis['dementia_status'])\n",
    "    print(\"Contingency table:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"\\nChi-square = {chi2:.4f}, P-value = {p_value:.4e}\")\n",
    "    \n",
    "    # 3. Logistic regression\n",
    "    print(\"\\n--- 3. Logistic regression analysis ---\")\n",
    "    \n",
    "    df_analysis['group_0'] = (df_analysis['group'] == 0).astype(int)\n",
    "    df_analysis['group_1'] = (df_analysis['group'] == 1).astype(int)\n",
    "    \n",
    "    # 3a. Unadjusted model\n",
    "    print(\"\\n3a. Unadjusted logistic regression (reference: Group 2):\")\n",
    "    \n",
    "    X_unadj = df_analysis[['group_0', 'group_1']]\n",
    "    X_unadj = sm.add_constant(X_unadj)\n",
    "    y = df_analysis['dementia_status']\n",
    "    \n",
    "    results_unadj = []\n",
    "    try:\n",
    "        model_unadj = sm.Logit(y, X_unadj).fit(disp=0)\n",
    "        \n",
    "        print(f\"\\n{'Group':<20} {'OR':<10} {'95% CI':<20} {'P-value':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for var in ['group_0', 'group_1']:\n",
    "            coef = model_unadj.params[var]\n",
    "            se = model_unadj.bse[var]\n",
    "            pval = model_unadj.pvalues[var]\n",
    "            or_val = np.exp(coef)\n",
    "            ci_low = np.exp(coef - 1.96 * se)\n",
    "            ci_high = np.exp(coef + 1.96 * se)\n",
    "            \n",
    "            group_name = \"Group 0 (Low protection)\" if var == 'group_0' else \"Group 1 (Medium)\"\n",
    "            sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"\"\n",
    "            print(f\"{group_name:<20} {or_val:<10.3f} ({ci_low:.3f}-{ci_high:.3f})    {pval:.4f} {sig}\")\n",
    "            \n",
    "            results_unadj.append({\n",
    "                'Model': 'Unadjusted', 'Group': group_name,\n",
    "                'OR': or_val, 'CI_low': ci_low, 'CI_high': ci_high, 'P_value': pval\n",
    "            })\n",
    "        \n",
    "        print(f\"{'Group 2 (High protection)':<20} {'1.000':<10} {'(Reference)':<20}\")\n",
    "        results_unadj.append({'Model': 'Unadjusted', 'Group': 'Group 2 (High protection)', \n",
    "                              'OR': 1.0, 'CI_low': 1.0, 'CI_high': 1.0, 'P_value': np.nan})\n",
    "    except Exception as e:\n",
    "        print(f\"Unadjusted logistic regression failed: {e}\")\n",
    "    \n",
    "    # 3b. Adjusted model\n",
    "    print(\"\\n3b. Adjusted logistic regression (covariates adjusted):\")\n",
    "    \n",
    "    adj_vars_for_model = [f'{var}_baseline' for var in available_adj_vars if f'{var}_baseline' in df_analysis.columns]\n",
    "    print(f\"Adjustment variables: {adj_vars_for_model}\")\n",
    "    \n",
    "    results_adj = []\n",
    "    df_adj = df_analysis.dropna(subset=adj_vars_for_model + ['group_0', 'group_1', 'dementia_status']).copy()\n",
    "    print(f\"Adjusted sample size: {len(df_adj)} individuals\")\n",
    "    \n",
    "    if len(df_adj) > 0 and len(adj_vars_for_model) > 0:\n",
    "        X_adj = df_adj[['group_0', 'group_1'] + adj_vars_for_model]\n",
    "        X_adj = sm.add_constant(X_adj)\n",
    "        y_adj = df_adj['dementia_status']\n",
    "        \n",
    "        try:\n",
    "            model_adj = sm.Logit(y_adj, X_adj).fit(disp=0)\n",
    "            \n",
    "            print(f\"\\n{'Variable':<25} {'OR':<10} {'95% CI':<20} {'P-value':<10}\")\n",
    "            print(\"-\" * 65)\n",
    "            \n",
    "            for var in ['group_0', 'group_1']:\n",
    "                coef = model_adj.params[var]\n",
    "                se = model_adj.bse[var]\n",
    "                pval = model_adj.pvalues[var]\n",
    "                or_val = np.exp(coef)\n",
    "                ci_low = np.exp(coef - 1.96 * se)\n",
    "                ci_high = np.exp(coef + 1.96 * se)\n",
    "                \n",
    "                group_name = \"Group 0 (Low protection)\" if var == 'group_0' else \"Group 1 (Medium)\"\n",
    "                sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"\"\n",
    "                print(f\"{group_name:<25} {or_val:<10.3f} ({ci_low:.3f}-{ci_high:.3f})    {pval:.4f} {sig}\")\n",
    "                \n",
    "                results_adj.append({\n",
    "                    'Model': 'Adjusted', 'Group': group_name,\n",
    "                    'OR': or_val, 'CI_low': ci_low, 'CI_high': ci_high, 'P_value': pval\n",
    "                })\n",
    "            \n",
    "            print(f\"{'Group 2 (High protection)':<25} {'1.000':<10} {'(Reference)':<20}\")\n",
    "            results_adj.append({'Model': 'Adjusted', 'Group': 'Group 2 (High protection)',\n",
    "                                'OR': 1.0, 'CI_low': 1.0, 'CI_high': 1.0, 'P_value': np.nan})\n",
    "            \n",
    "            print(\"\\nCovariate effects:\")\n",
    "            for var in adj_vars_for_model:\n",
    "                if var in model_adj.params.index:\n",
    "                    or_val = np.exp(model_adj.params[var])\n",
    "                    pval = model_adj.pvalues[var]\n",
    "                    sig = \"***\" if pval < 0.001 else \"**\" if pval < 0.01 else \"*\" if pval < 0.05 else \"\"\n",
    "                    print(f\"  {var}: OR = {or_val:.3f}, P = {pval:.4f} {sig}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Adjusted logistic regression failed: {e}\")\n",
    "    \n",
    "    # 4. Save logistic regression results\n",
    "    all_results = results_unadj + results_adj\n",
    "    if all_results:\n",
    "        df_results = pd.DataFrame(all_results)\n",
    "        results_csv = os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_group_dementia_logistic_results.csv')\n",
    "        df_results.to_csv(results_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSaved: {results_csv}\")\n",
    "    \n",
    "    # 5. Forest plot\n",
    "    print(\"\\n--- 5. Forest plot ---\")\n",
    "    \n",
    "    plot_data = []\n",
    "    for r in results_unadj + results_adj:\n",
    "        if r['OR'] != 1.0:\n",
    "            plot_data.append(r)\n",
    "    \n",
    "    if plot_data:\n",
    "        df_plot = pd.DataFrame(plot_data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        colors = {'Unadjusted': '#1f77b4', 'Adjusted': '#ff7f0e'}\n",
    "        \n",
    "        y_pos = 0\n",
    "        y_positions = []\n",
    "        y_labels = []\n",
    "        \n",
    "        for group in ['Group 0 (Low protection)', 'Group 1 (Medium)']:\n",
    "            for model in ['Unadjusted', 'Adjusted']:\n",
    "                row = df_plot[(df_plot['Group'] == group) & (df_plot['Model'] == model)]\n",
    "                if len(row) > 0:\n",
    "                    row = row.iloc[0]\n",
    "                    y_positions.append(y_pos)\n",
    "                    y_labels.append(f\"{group}\\n{model}\")\n",
    "                    \n",
    "                    ax.plot([row['CI_low'], row['CI_high']], [y_pos, y_pos], \n",
    "                            color=colors[model], linewidth=2)\n",
    "                    ax.scatter(row['OR'], y_pos, color=colors[model], s=100, zorder=5)\n",
    "                    \n",
    "                    sig = '***' if row['P_value'] < 0.001 else '**' if row['P_value'] < 0.01 else '*' if row['P_value'] < 0.05 else ''\n",
    "                    ax.annotate(f\"{row['OR']:.2f}{sig}\", xy=(row['CI_high'] + 0.1, y_pos), fontsize=10, va='center')\n",
    "                    \n",
    "                    y_pos += 1\n",
    "            y_pos += 0.5\n",
    "        \n",
    "        ax.axvline(x=1, color='black', linestyle='--', linewidth=1)\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(y_labels)\n",
    "        ax.set_xlabel('Odds Ratio (95% CI)', fontsize=12)\n",
    "        ax.set_title(f'{DATASET_NAME}: Trajectory Group and Dementia Risk\\n(Reference: Group 2 - High Protection)', fontsize=14)\n",
    "        ax.set_xlim(0, max(df_plot['CI_high'].max() * 1.3, 3))\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='#1f77b4', label='Unadjusted'),\n",
    "                           Patch(facecolor='#ff7f0e', label='Adjusted')]\n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_group_dementia_forest_plot.pdf'),\n",
    "                    format='pdf', bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_group_dementia_forest_plot.png'),\n",
    "                    format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Saved: forest_plot\")\n",
    "    \n",
    "    # 6. Bar plot\n",
    "    print(\"\\n--- 6. Bar plot ---\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    group_labels = ['Group 0\\n(Low Protection)', 'Group 1\\n(Medium)', 'Group 2\\n(High Protection)']\n",
    "    dementia_rates = dementia_by_group['dementia_pct'].values\n",
    "    colors = ['#d62728', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    bars = ax.bar(group_labels, dementia_rates, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    for bar, rate, (idx, row) in zip(bars, dementia_rates, dementia_by_group.iterrows()):\n",
    "        ax.annotate(f'{rate:.1f}%\\n(n={int(row[\"dementia_cases\"])}/{int(row[\"n\"])})',\n",
    "                    xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                    ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    ax.set_ylabel('Dementia Incidence Rate (%)', fontsize=12)\n",
    "    ax.set_title(f'{DATASET_NAME}: Dementia Incidence by {SCORE_VAR} Trajectory Group', fontsize=14)\n",
    "    ax.set_ylim(0, max(dementia_rates) * 1.3)\n",
    "    ax.annotate(f'Chi-square: P = {p_value:.4e}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "                ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_group_dementia_barplot.pdf'),\n",
    "                format='pdf', bbox_inches='tight')\n",
    "    plt.savefig(os.path.join(output_dir, f'{DATASET_NAME}_{SCORE_VAR}_group_dementia_barplot.png'),\n",
    "                format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved: barplot\")\n",
    "\n",
    "# =========================\n",
    "# Plot trajectories\n",
    "# =========================\n",
    "print(\"\\n=== Plotting trajectories ===\")\n",
    "\n",
    "traj_data = []\n",
    "for i, id_val in enumerate(unique_ids):\n",
    "    group = cluster_labels[i]\n",
    "    for t_idx, t_val in enumerate(time_grid):\n",
    "        traj_data.append({\n",
    "            'id': id_val, 'wave': t_val,\n",
    "            f'{SCORE_VAR}_interp': interpolated_original[i, t_idx],\n",
    "            'group': group\n",
    "        })\n",
    "df_traj = pd.DataFrame(traj_data)\n",
    "\n",
    "palette_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "unique_groups = sorted(df['group'].dropna().unique().tolist())\n",
    "color_map = {g: palette_colors[g % len(palette_colors)] for g in unique_groups}\n",
    "\n",
    "# Trajectory plot 1: Mean trajectories\n",
    "plt.figure(figsize=(10, 6))\n",
    "for g in unique_groups:\n",
    "    gdata = df[df['group'] == g]\n",
    "    n_g = gdata['id'].nunique()\n",
    "    sns.lineplot(data=gdata, x='wave', y=SCORE_VAR, color=color_map[g], \n",
    "                 errorbar=('ci', 95), linewidth=2.5, label=f'Group {g} (n={n_g})')\n",
    "plt.xlim(WAVE_MIN, WAVE_MAX)\n",
    "plt.xlabel('Wave', fontsize=12)\n",
    "plt.ylabel(SCORE_VAR, fontsize=12)\n",
    "plt.title(f'{DATASET_NAME}: Mean {SCORE_VAR} Trajectories by Group (k=3)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Trajectory Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'mean_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.pdf'),\n",
    "            format='pdf', bbox_inches='tight')\n",
    "plt.savefig(os.path.join(output_dir, f'mean_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.png'),\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Trajectory plot 2: Smoothed mean trajectories\n",
    "plt.figure(figsize=(10, 6))\n",
    "for g in unique_groups:\n",
    "    gdata = df_traj[df_traj['group'] == g]\n",
    "    n_g = gdata['id'].nunique()\n",
    "    sns.lineplot(data=gdata, x='wave', y=f'{SCORE_VAR}_interp', color=color_map[g],\n",
    "                 errorbar=('ci', 95), linewidth=2.5, label=f'Group {g} (n={n_g})')\n",
    "plt.xlim(WAVE_MIN, WAVE_MAX)\n",
    "plt.xlabel('Wave', fontsize=12)\n",
    "plt.ylabel(f'{SCORE_VAR} (Interpolated)', fontsize=12)\n",
    "plt.title(f'{DATASET_NAME}: Smoothed {SCORE_VAR} Trajectories (k=3)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Trajectory Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'smooth_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.pdf'),\n",
    "            format='pdf', bbox_inches='tight')\n",
    "plt.savefig(os.path.join(output_dir, f'smooth_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.png'),\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Trajectory plot 3: Individual trajectories\n",
    "plt.figure(figsize=(10, 6))\n",
    "for g in unique_groups:\n",
    "    gdata = df[df['group'] == g]\n",
    "    for uid in gdata['id'].unique():\n",
    "        uid_data = gdata[gdata['id'] == uid].sort_values('wave')\n",
    "        plt.plot(uid_data['wave'], uid_data[SCORE_VAR], color=color_map[g], alpha=0.1, linewidth=0.5)\n",
    "    n_g = gdata['id'].nunique()\n",
    "    sns.lineplot(data=gdata, x='wave', y=SCORE_VAR, color=color_map[g],\n",
    "                 errorbar=None, linewidth=3, label=f'Group {g} (n={n_g})')\n",
    "plt.xlim(WAVE_MIN, WAVE_MAX)\n",
    "plt.xlabel('Wave', fontsize=12)\n",
    "plt.ylabel(SCORE_VAR, fontsize=12)\n",
    "plt.title(f'{DATASET_NAME}: Individual {SCORE_VAR} Trajectories by Group (k=3)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Trajectory Group')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'individual_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.pdf'),\n",
    "            format='pdf', bbox_inches='tight')\n",
    "plt.savefig(os.path.join(output_dir, f'individual_trajectories_{DATASET_NAME}_{SCORE_VAR}_original_k3.png'),\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Silhouette scores\n",
    "sil_df = pd.DataFrame(sil_scores, columns=['k', 'silhouette'])\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sil_df['k'], sil_df['silhouette'], 'o-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=FIXED_K, color='red', linestyle='--', label=f'Selected k={FIXED_K}')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "plt.ylabel('Silhouette Score', fontsize=12)\n",
    "plt.title(f'{DATASET_NAME}: Silhouette Score by Number of Clusters', fontsize=14)\n",
    "plt.xticks(list(k_range))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, f'silhouette_scores_{DATASET_NAME}_{SCORE_VAR}_adjusted_k3.pdf'),\n",
    "            format='pdf', bbox_inches='tight')\n",
    "plt.savefig(os.path.join(output_dir, f'silhouette_scores_{DATASET_NAME}_{SCORE_VAR}_adjusted_k3.png'),\n",
    "            format='png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nTrajectory plots saved\")\n",
    "\n",
    "# =========================\n",
    "# Completion\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"=== {DATASET_NAME} processing completed ===\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n★ Group interpretation:\")\n",
    "print(\"  Group 0 = Low level (low protection → high dementia risk)\")\n",
    "print(\"  Group 1 = Medium level\")\n",
    "print(\"  Group 2 = High level (high protection → low dementia risk, reference)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (heart_aging)",
   "language": "python",
   "name": "heart_aging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
