{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a3a3f-a1a7-4826-bdbb-d5a0e25fc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Ordinal Deep Learning Model for T2D-Dementia Fluctuating_Decreasing Cluster\n",
    "- Uses grid search to find optimal architecture and hyperparameters\n",
    "- Applies SMOTE oversampling to handle class imbalance\n",
    "- Implements custom ordinal regression loss with monotonic threshold constraints\n",
    "- Evaluates using classification report and MAE\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "from scipy.stats import bootstrap\n",
    "import itertools\n",
    "\n",
    "# ============================= Configuration =============================\n",
    "DATA_PATH = \"./data/extracted_Gradually_Increasing.csv\" # (example uses GI dataset)\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# ============================= 1. Data Preparation =============================\n",
    "print(\"\\n1. Loading and preprocessing data...\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "protein_names = df.columns[9:76].tolist()\n",
    "covariates_names = df.columns[2:9].tolist()\n",
    "num_proteins = len(protein_names)\n",
    "\n",
    "# Extract features and target\n",
    "y = df.iloc[:, 1].values.astype(np.int32)\n",
    "X_protein = df.iloc[:, 9:76].values.astype(np.float32)\n",
    "X_covariates = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_protein = imputer.fit_transform(X_protein)\n",
    "X_covariates = imputer.fit_transform(X_covariates)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_protein_scaled = scaler.fit_transform(X_protein)\n",
    "X_covariates_scaled = scaler.fit_transform(X_covariates)\n",
    "X_combined = np.hstack([X_protein_scaled, X_covariates_scaled])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Apply SMOTE oversampling\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Further split training data into sub-train and validation for grid search\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Define grid search space\n",
    "layer_configs = [[1024, 512, 256, 128], [512, 256, 128], [256, 128]]\n",
    "dropout_rates = [0.2, 0.3, 0.4]\n",
    "l2_lambdas = [0.001, 0.01, 0.1]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]\n",
    "\n",
    "# Custom monotonic constraint for thresholds\n",
    "class MonotonicConstraint(tf.keras.constraints.Constraint):\n",
    "    \"\"\"Enforce monotonic increasing constraint on ordinal thresholds\"\"\"\n",
    "    def __call__(self, w):\n",
    "        return tf.cumsum(tf.nn.elu(w) + 1e-6)\n",
    "\n",
    "# Model builder with configurable hyperparameters\n",
    "def build_model(input_dim, num_classes, layer_sizes, dropout_rate, l2_lambda, learning_rate):\n",
    "    class DeepOrdinal(tf.keras.Model):\n",
    "        def __init__(self, input_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # Feature extraction backbone\n",
    "            self.dense_stack = tf.keras.Sequential()\n",
    "            for size in layer_sizes:\n",
    "                self.dense_stack.add(tf.keras.layers.Dense(\n",
    "                    size, activation='relu',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)\n",
    "                ))\n",
    "                self.dense_stack.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "            \n",
    "            # Output layer: logits before thresholds\n",
    "            self.output_layer = tf.keras.layers.Dense(\n",
    "                num_classes - 1,\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)\n",
    "            )\n",
    "            \n",
    "            # Trainable monotonic thresholds\n",
    "            self.thresholds = tf.Variable(\n",
    "                initial_value=tf.sort(tf.linspace(-1.0, 1.0, num_classes - 1)),\n",
    "                trainable=True,\n",
    "                constraint=MonotonicConstraint(),\n",
    "                name=\"ordinal_thresholds\"\n",
    "            )\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense_stack(inputs)\n",
    "            return self.output_layer(x)\n",
    "\n",
    "        def custom_loss(self, y_true, y_pred):\n",
    "            \"\"\"Custom ordinal regression loss with cumulative logits\"\"\"\n",
    "            y_true = tf.cast(tf.reshape(y_true, (-1, 1)), tf.float32)\n",
    "            cum_loss = 0.0\n",
    "            \n",
    "            for k in range(self.num_classes - 1):\n",
    "                target = tf.cast(y_true > k, tf.float32)\n",
    "                logit = y_pred[:, k] - self.thresholds[k]\n",
    "                \n",
    "                # Numerical stability\n",
    "                logit = tf.clip_by_value(logit, -10.0, 10.0)\n",
    "                logit = tf.reshape(logit, (-1, 1))\n",
    "                \n",
    "                loss = tf.nn.sigmoid_cross_entropy_with_logits(target, logit)\n",
    "                cum_loss += tf.reduce_mean(loss)\n",
    "                \n",
    "            return cum_loss\n",
    "\n",
    "    model = DeepOrdinal(input_dim=input_dim, num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        clipvalue=1.0  # Gradient clipping\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=model.custom_loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "best_score = float('inf')  # Minimize validation MAE\n",
    "best_params = None\n",
    "results = []  # Track all configurations\n",
    "\n",
    "for layer_sizes, dropout_rate, l2_lambda, lr in itertools.product(\n",
    "    layer_configs, dropout_rates, l2_lambdas, learning_rates\n",
    "):\n",
    "    print(f\"Evaluating config: layers={layer_sizes}, dropout={dropout_rate}, \"\n",
    "          f\"l2={l2_lambda}, lr={lr}\")\n",
    "    \n",
    "    model = build_model(\n",
    "        input_dim=X_subtrain.shape[1],\n",
    "        num_classes=5,\n",
    "        layer_sizes=layer_sizes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l2_lambda=l2_lambda,\n",
    "        learning_rate=lr\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_subtrain, y_subtrain,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set and compute MAE\n",
    "    y_val_pred = model.predict(X_val)  # logits\n",
    "    # Convert logits to class predictions (simple argmax after cumulative sum)\n",
    "    cum_probs = tf.sigmoid(y_val_pred - model.thresholds)\n",
    "    y_val_pred_classes = tf.argmax(cum_probs, axis=1).numpy()\n",
    "    \n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred_classes)\n",
    "    \n",
    "    results.append({\n",
    "        'layers': layer_sizes,\n",
    "        'dropout': dropout_rate,\n",
    "        'l2': l2_lambda,\n",
    "        'lr': lr,\n",
    "        'val_mae': val_mae\n",
    "    })\n",
    "    \n",
    "    if val_mae < best_score:\n",
    "        best_score = val_mae\n",
    "        best_params = {\n",
    "            'layers': layer_sizes,\n",
    "            'dropout': dropout_rate,\n",
    "            'l2': l2_lambda,\n",
    "            'lr': lr\n",
    "        }\n",
    "\n",
    "# Save grid search results\n",
    "pd.DataFrame(results).to_csv('./plot/Gradually_Increasing/grid_search_results.csv', index=False)\n",
    "\n",
    "print(f\"Best configuration: {best_params} with validation MAE: {best_score:.4f}\")\n",
    "\n",
    "# ============================= 2. Final Model Initialization =============================\n",
    "print(\"\\n2. Training final model with best configuration...\")\n",
    "\n",
    "final_model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=5,\n",
    "    layer_sizes=best_params['layers'],\n",
    "    dropout_rate=best_params['dropout'],\n",
    "    l2_lambda=best_params['l2'],\n",
    "    learning_rate=best_params['lr']\n",
    ")\n",
    "\n",
    "# ============================= 3. Final Model Training =============================\n",
    "history = final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ============================= 4. Model Evaluation =============================\n",
    "print(\"\\n3. Evaluating final model...\")\n",
    "\n",
    "# Get predictions (logits → class)\n",
    "val_logits = final_model.predict(X_test)\n",
    "cum_probs = tf.sigmoid(val_logits - final_model.thresholds)\n",
    "y_pred = tf.argmax(cum_probs, axis=1).numpy()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nMean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df7fa7-1a73-41cc-9312-5f84d12d4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5. Gradient-based Feature Importance (Protein Features Only) =====================\n",
    "print(\"\\n5. Computing gradient-based importance...\")\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)  # Explicitly watch inputs\n",
    "        preds = model(inputs)\n",
    "    return tape.gradient(preds, inputs)\n",
    "\n",
    "# Compute gradients for all training samples\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "gradients = compute_gradients(X_train_tensor)\n",
    "\n",
    "if gradients is None:\n",
    "    raise ValueError(\"Gradient computation failed. Please check model input-output dependency.\")\n",
    "\n",
    "# Take absolute values and extract protein part\n",
    "abs_gradients = tf.abs(gradients).numpy()  # Convert to NumPy for further processing\n",
    "protein_abs_gradients = abs_gradients[:, :num_proteins]  # First num_proteins columns are proteins\n",
    "\n",
    "# Bootstrapping function: mean importance per protein\n",
    "def bootstrap_mean(data):\n",
    "    return np.mean(data, axis=0)  # Mean across samples for each feature\n",
    "\n",
    "# Perform bootstrapping (95% CI, 1000 resamples)\n",
    "boot_result = bootstrap(\n",
    "    (protein_abs_gradients,),\n",
    "    bootstrap_mean,\n",
    "    n_resamples=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    method='percentile'\n",
    ")\n",
    "\n",
    "# Extract bootstrap statistics\n",
    "gradient_importance_mean = boot_result.bootstrap_distribution.mean(axis=-1)\n",
    "gradient_importance_se   = boot_result.standard_error\n",
    "gradient_importance_ci_low  = boot_result.confidence_interval.low\n",
    "gradient_importance_ci_high = boot_result.confidence_interval.high\n",
    "\n",
    "# Diagnostic prints (optional — can be removed after verification)\n",
    "print(\"Number of protein features:\", len(protein_names))\n",
    "print(\"Shape of gradient_importance_mean:\", gradient_importance_mean.shape)\n",
    "print(\"Shape of gradient_importance_se:\", gradient_importance_se.shape)\n",
    "print(\"Shape of gradient_importance_ci_low:\", gradient_importance_ci_low.shape)\n",
    "print(\"Shape of gradient_importance_ci_high:\", gradient_importance_ci_high.shape)\n",
    "\n",
    "# Save results to CSV with confidence intervals\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': gradient_importance_mean,\n",
    "    'Gradient Importance SE': gradient_importance_se,\n",
    "    'Gradient Importance CI Lower (95%)': gradient_importance_ci_low,\n",
    "    'Gradient Importance CI Upper (95%)': gradient_importance_ci_high\n",
    "}).to_csv(\n",
    "    './plot/Gradually_Increasing/gradient_importance_dementia_with_ci.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Gradient importance results (with 95% CI) saved to: \"\n",
    "      \"./plot/Gradually_Increasing/gradient_importance_dementia_with_ci.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfa7e2-f1fe-4fbb-95f9-340753e5e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Permutation Test Parameters\n",
    "n_permutations = 1000  # Adjust for computational feasibility\n",
    "\n",
    "# Function to compute mean absolute gradients\n",
    "def compute_mean_abs_gradients(inputs):\n",
    "    grads = compute_gradients(tf.convert_to_tensor(inputs, dtype=tf.float32))\n",
    "    return np.mean(np.abs(grads.numpy()), axis=0)[:num_proteins]  # Protein part only\n",
    "\n",
    "# Original mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Generate null distribution\n",
    "null_distribution = np.zeros((n_permutations, num_proteins))\n",
    "for i in range(n_permutations):\n",
    "    # Permute features (columns) independently for each protein\n",
    "    permuted_X = X_train.copy()\n",
    "    for j in range(num_proteins):\n",
    "        np.random.shuffle(permuted_X[:, j])\n",
    "    null_distribution[i] = compute_mean_abs_gradients(permuted_X)\n",
    "\n",
    "# Compute p-values (one-tailed: proportion of null means >= original)\n",
    "p_values_perm = np.array([1 - (percentileofscore(null_distribution[:, j], original_mean[j]) / 100) for j in range(num_proteins)])\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Gradient Importance Mean': original_mean,\n",
    "    'P-value (Permutation)': p_values_perm\n",
    "}).to_csv('./plot/Gradually_Increasing/gradient_importance_permutation_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb0b8b-1261-4ad2-8b2b-56b4b1a8d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified; insert after model training\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Compute mean importance\n",
    "original_mean = np.mean(protein_abs_gradients, axis=0)\n",
    "\n",
    "# Approximate pivotal statistic (e.g., standardized gradient mean)\n",
    "std_gradients = np.std(protein_abs_gradients, axis=0)\n",
    "pivotal_stats = original_mean / (std_gradients / np.sqrt(protein_abs_gradients.shape[0]))\n",
    "p_values_pivotal = 2 * (1 - norm.cdf(np.abs(pivotal_stats)))  # Two-tailed\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'Protein': protein_names,\n",
    "    'Pivotal Statistic': pivotal_stats,\n",
    "    'P-value (Pivotal)': p_values_pivotal\n",
    "}).to_csv('./plot/Gradually_Increasing/gradient_importance_pivotal_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
